{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltlmEzQaUAVy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltlmEzQaUAVy",
    "outputId": "f904c860-29af-426a-ffb8-e0a548f4ae20"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb3b24",
   "metadata": {
    "id": "7ceb3b24"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import base64\n",
    "import array\n",
    "import glob\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341151b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c341151b",
    "outputId": "da661004-c0c5-4a5b-f4b1-781c16aa5170"
   },
   "outputs": [],
   "source": [
    "def load_test_img(img_folder):\n",
    "    IMG_WIDTH=480\n",
    "    IMG_HEIGHT=256\n",
    "    img_a=[]\n",
    "    img_a2=[]\n",
    "    img_size=[]\n",
    "    img_size2=[]\n",
    "    i=0\n",
    "    for filename in glob.iglob(img_folder + '**/*.png', recursive=True):\n",
    "        img1= cv2.imread( filename, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        name=os.path.basename(filename)\n",
    "        name=os.path.splitext(name)[0]\n",
    "        _1, img_arr1 = cv2.imencode('.png', img1)\n",
    "        img_bytes1 = img_arr1.tobytes()\n",
    "        img_b641 = base64.b64encode(img_bytes1)\n",
    "        \n",
    "        #---------------------------------------------\n",
    "        \n",
    "        img_b=base64.decodebytes(img_b641)\n",
    "        \n",
    "        img_ar = np.frombuffer(img_b, dtype=np.uint8)\n",
    "        \n",
    "        img_ = cv2.imdecode(img_ar, flags=cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        height, width = img_.shape[:2]\n",
    "        img_size.append([width,height,name])\n",
    "        img_=cv2.resize(img_,dsize=(IMG_WIDTH,IMG_HEIGHT),interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        img_=img_/255\n",
    "        img_a.append(img_)\n",
    "        \n",
    "        i+=1\n",
    "    return img_a,img_size\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    img_data_o,data_size_o=load_test_img('/home/eduardo/Desktop/TesteRoit/original/1test/')\n",
    "    img_data_o=np.array(img_data_o)\n",
    "    img_data_o = img_data_o[..., tf.newaxis]\n",
    "    print(img_data_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868ea05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6868ea05",
    "outputId": "a6b21315-e34b-48d6-9654-eba3785adbe9"
   },
   "outputs": [],
   "source": [
    "# BASE 64 - Byte Array 2\n",
    "\n",
    "def create_dataset(img_folder,img_folder2):\n",
    "    IMG_WIDTH=480\n",
    "    IMG_HEIGHT=256\n",
    "    img_a=[]\n",
    "    img_a2=[]\n",
    "    img_size=[]\n",
    "    img_size2=[]\n",
    "    i=0\n",
    "    for filename in glob.iglob(img_folder + '**/*.png', recursive=True):\n",
    "        img1= cv2.imread( filename, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        name=os.path.basename(filename)\n",
    "        name=os.path.splitext(name)[0]\n",
    "        _1, img_arr1 = cv2.imencode('.png', img1)\n",
    "        img_bytes1 = img_arr1.tobytes()\n",
    "        img_b641 = base64.b64encode(img_bytes1)\n",
    "        \n",
    "        img2= cv2.imread(img_folder2+name+\".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        _2, img_arr2 = cv2.imencode('.png', img2)\n",
    "        img_bytes2 = img_arr2.tobytes()\n",
    "        img_b642 = base64.b64encode(img_bytes2)\n",
    "        \n",
    "        \n",
    "        #---------------------------------------------\n",
    "        \n",
    "        img_b=base64.decodebytes(img_b641)\n",
    "        img_ar = np.frombuffer(img_b, dtype=np.uint8)\n",
    "        \n",
    "        img_ = cv2.imdecode(img_ar, flags=cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        height, width = img_.shape[:2]\n",
    "        img_size.append([width,height,name])\n",
    "        img_=cv2.resize(img_,dsize=(IMG_WIDTH,IMG_HEIGHT),interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        img_=img_/255\n",
    "        img_a.append(img_)\n",
    "        \n",
    "        #---2----\n",
    "        \n",
    "        img_b2=base64.decodebytes(img_b642)\n",
    "        img_ar2 = np.frombuffer(img_b2, dtype=np.uint8)\n",
    "        \n",
    "        img_2 = cv2.imdecode(img_ar2, flags=cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        height2, width2 = img_2.shape[:2]\n",
    "        img_size2.append([width2,height2,name])\n",
    "        img_2=cv2.resize(img_2,dsize=(IMG_WIDTH,IMG_HEIGHT),interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        img_2=img_2/255\n",
    "        img_a2.append(img_2)\n",
    "\n",
    "        i+=1\n",
    "    return img_a,img_a2,img_size,img_size2\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    img_data,img_clear,data_size,data_size_c=create_dataset('/home/eduardo/Desktop/TesteRoit/original/1dirty/',\n",
    "                                        '/home/eduardo/Desktop/TesteRoit/original/2cleaned_example/')\n",
    "\n",
    "    img_data=np.array(img_data)\n",
    "    img_clear=np.array(img_clear)\n",
    "\n",
    "    img_data = img_data[..., tf.newaxis]\n",
    "    img_clear = img_clear[..., tf.newaxis]\n",
    "\n",
    "    print(img_data.shape)\n",
    "    print(img_clear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ce30a",
   "metadata": {
    "id": "6e2ce30a"
   },
   "outputs": [],
   "source": [
    "n =2\n",
    "i=0\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(n):\n",
    "  # display original\n",
    "  ax = plt.subplot(2, n, i + 1)\n",
    "  plt.imshow(img_clear[i+50])\n",
    "  plt.title(\"Clear\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "  # display reconstruction\n",
    "  ax = plt.subplot(2, n, i + 1 + n)\n",
    "  plt.imshow(img_data[i+50])\n",
    "  plt.title(\"Dirtys\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdbbe65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cdbbe65",
    "outputId": "0613d073-7b6c-4685-b7a8-b48f257898f3"
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    img_train_c, img_test_c,img_train_d,img_test_d = train_test_split(img_clear,img_data, test_size=0.2,random_state=42)\n",
    "    print(img_train_c.shape)\n",
    "    print(img_test_c.shape)\n",
    "    print(img_train_d.shape)\n",
    "    print(img_test_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39407f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,path):\n",
    "    model.save(path+\"autoencoder\",save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2afd45",
   "metadata": {
    "id": "6e2afd45"
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            save_model(self.model,'/home/eduardo/Desktop/TesteRoit/Models/ESM/')\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ddaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.drop=0.2\n",
    "        self.data=data\n",
    "        self.cp_path='/home/eduardo/Desktop/TesteRoit/Models/'\n",
    "        self.f1=128\n",
    "        self.f2=64\n",
    "        super(Autoencoder, self).__init__()  \n",
    "        \n",
    "        log_dir = \"/home/eduardo/Desktop/TesteRoit/Models/tensorboard/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        self.cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self.cp_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 save_freq='epoch',period=25)\n",
    "        self.batch_print_callback = LambdaCallback(on_epoch_end=self.test)\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "        layers.Input(shape=(256, 480, 1)),\n",
    "        layers.Conv2D(self.f1, 3, strides=2, padding='same', name='cv1'),\n",
    "        layers.BatchNormalization(name='bn_1'),\n",
    "        layers.LeakyReLU(name='lrelu_1'),\n",
    "        layers.Dropout(rate = self.drop),\n",
    "            \n",
    "        layers.Conv2D(self.f2, 3, strides=2, padding='same', name='cv2'),\n",
    "        layers.BatchNormalization(name='bn_2'),\n",
    "        layers.LeakyReLU(name='lrelu_2'),\n",
    "        layers.Dropout(rate = self.drop),\n",
    "\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([  \n",
    " \n",
    "        layers.Conv2DTranspose(self.f2, kernel_size=3, strides=2,activation='relu', padding='same', name='cv_transpose_3'),\n",
    "        layers.BatchNormalization(name='bn_3'),\n",
    "        layers.LeakyReLU(name='lrelu_3'),\n",
    "        layers.Dropout(rate = self.drop),\n",
    "            \n",
    "        layers.Conv2DTranspose(self.f1, kernel_size=3, strides=2,activation='relu',padding='same', name='cv_transpose_4'),\n",
    "        layers.BatchNormalization(name='bn_4'),\n",
    "        layers.LeakyReLU(name='lrelu_4'),\n",
    "        layers.Dropout(rate = self.drop),\n",
    "            \n",
    "        layers.Conv2DTranspose(1, kernel_size=3, strides= 1,padding='same', activation='sigmoid', name='cv_transpose_6')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def test(self,epoch,logs={}):\n",
    "\n",
    "        x = self.data    \n",
    "        if not epoch%20:\n",
    "            print(\"Save Decoded\")\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = self(x, training=True)  \n",
    "                y_pred=np.array(y_pred)\n",
    "                y_pred=y_pred*255\n",
    "                for i in range(len(y_pred)):\n",
    "                    cv2.imwrite(\"/home/eduardo/Desktop/TesteRoit/decoded/\"+str(epoch)+\"-\"+str(i)+\".png\",y_pred[i])\n",
    "        return None\n",
    "    \n",
    "with tf.device('/CPU:0'):  \n",
    "    autoencoder = Autoencoder(img_data_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b68b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a7b68b2",
    "outputId": "758e3917-0c68-487d-8f64-184f8985338a"
   },
   "outputs": [],
   "source": [
    "# autoencoder.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e58c3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06e58c3b",
    "outputId": "0ada8e0b-a844-495c-df92-6c9755355572"
   },
   "outputs": [],
   "source": [
    "# autoencoder.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9bbca",
   "metadata": {
    "id": "33b9bbca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f0809",
   "metadata": {
    "id": "3a0f0809",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d858e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb8d858e",
    "outputId": "1979628e-4a14-4ce0-9ce5-2c574bfa100b"
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    print(img_data.shape)\n",
    "    print(img_clear.shape)\n",
    "\n",
    "    autoencoder.fit(img_train_d,img_train_c ,\n",
    "                    epochs=15000,\n",
    "                    shuffle=True,\n",
    "                    batch_size=8,\n",
    "                    validation_data=(img_test_d,img_test_c),\n",
    "                    callbacks=[EarlyStoppingAtMinLoss(70),\n",
    "                               autoencoder.batch_print_callback,\n",
    "                               autoencoder.cp_callback,\n",
    "                               autoencoder.tensorboard_callback\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598bab87",
   "metadata": {
    "id": "598bab87"
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "save_model(autoencoder,'/home/eduardo/Desktop/TesteRoit/Models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f3c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3be566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = tf.keras.models.load_model(path) # Load the model\n",
    "    return model\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    autoencoder_r=load_model('/home/eduardo/Desktop/TesteRoit/Models/ESM/autoencoder/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71029385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_img(img_folder):\n",
    "    IMG_WIDTH=480\n",
    "    IMG_HEIGHT=256\n",
    "    img_a=[]\n",
    "    img_a2=[]\n",
    "    img_size=[]\n",
    "    img_size2=[]\n",
    "    i=0\n",
    "    for filename in glob.iglob(img_folder + '**/*.png', recursive=True):\n",
    "        img1= cv2.imread( filename, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        name=os.path.basename(filename)\n",
    "        name=os.path.splitext(name)[0]\n",
    "#         img=cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "        _1, img_arr1 = cv2.imencode('.png', img1)\n",
    "        img_bytes1 = img_arr1.tobytes()\n",
    "        img_b641 = base64.b64encode(img_bytes1)\n",
    "        \n",
    "        #---------------------------------------------\n",
    "        \n",
    "        img_b=base64.decodebytes(img_b641)\n",
    "        img_ar = np.frombuffer(img_b, dtype=np.uint8)\n",
    "        \n",
    "        img_ = cv2.imdecode(img_ar, flags=cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        height, width = img_.shape[:2]\n",
    "        img_size.append([width,height,name])\n",
    "        img_=cv2.resize(img_,dsize=(IMG_WIDTH,IMG_HEIGHT),interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        img_=img_/255\n",
    "        img_a.append(img_)\n",
    "        \n",
    "        i+=1\n",
    "    return img_a,img_size\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    img_data_t,data_size_t=load_test_img('/home/eduardo/Desktop/TesteRoit/original/1test/')\n",
    "    img_data_t=np.array(img_data_t)\n",
    "    img_data_t = img_data_t[..., tf.newaxis]\n",
    "    print(img_data_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6daba",
   "metadata": {
    "id": "75b6daba"
   },
   "outputs": [],
   "source": [
    "def process_image(model,img_data_in,path_out,data_size_in):\n",
    "\n",
    "    encoded_imgs=[]\n",
    "    decoded_imgs=[]\n",
    "    img_data_d=[]\n",
    "    img_decoded_d=[]\n",
    "\n",
    "    for i in range(len(img_data_t)):\n",
    "        img_data=img_data_in[i]\n",
    "        img_data = img_data[np.newaxis,...]\n",
    "        encoded_imgs = autoencoder_r.encoder(img_data).numpy()\n",
    "        decoded_imgs = autoencoder_r.decoder(encoded_imgs).numpy()\n",
    "        \n",
    "        img_data_d.append(cv2.resize(img_data_in[i], dsize=(data_size_in[i][0],data_size_in[i][1]),interpolation = cv2.INTER_AREA))\n",
    "        img_decoded_d.append(cv2.resize(decoded_imgs[0], dsize=(data_size_t[i][0],data_size_t[i][1]),interpolation = cv2.INTER_AREA))\n",
    "    \n",
    "    img_data_d=np.array(img_data_d)*255\n",
    "    img_decoded_d=np.array(img_decoded_d)*255\n",
    "    \n",
    "    print(img_data_d.shape)\n",
    "    print(img_decoded_d.shape)\n",
    "    \n",
    "    return(img_data_d,img_decoded_d)\n",
    "  \n",
    "    \n",
    "with tf.device('/GPU:0'):\n",
    "    img_dirty,img_cleaned=process_image(autoencoder_r,img_data_t,None,data_size_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1778c",
   "metadata": {
    "id": "52c1778c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n =5\n",
    "i=0\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(n):\n",
    "  # display original\n",
    "  ax = plt.subplot(2, n, i + 1)\n",
    "  plt.imshow(img_dirty[i])\n",
    "  plt.title(\"original\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "  # display reconstruction\n",
    "  ax = plt.subplot(2, n, i + 1 + n)\n",
    "  plt.imshow(img_cleaned[i])\n",
    "  plt.title(\"reconstructed\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a58ef",
   "metadata": {
    "id": "595a58ef"
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_decoded_imgs(path,imgs):\n",
    "    for x in range(len(imgs)):\n",
    "#         print(data_size_o[x][2])\n",
    "        cv2.imwrite(path+data_size_t[x][2]+\".png\",imgs[x])\n",
    "                    \n",
    "save_decoded_imgs('/home/eduardo/Desktop/TesteRoit/data_decoded/',img_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477235b",
   "metadata": {
    "id": "b477235b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoderCONV2D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
